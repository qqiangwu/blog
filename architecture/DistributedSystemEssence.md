**注意, 以下内容有讹误, 请带着脑子阅读!**

# 分布式系统简介
这里所指的分布式系统, 特指使用集群技术来实现高可用高伸缩高性能的分布式系统.

## 基础概念
### 两个组件
为了方便研究分布式系统, 我们对分布式系统进行分解. 一个典型的分布式系统内存在两种实体, 一是执行具体任务的Node, 另一个是将所以Node连接起来的Link. 整个分布式系统表现为由若干Link相连接的若干Node的拓扑.

+ Node: Node是分布式系统中任务的单位, 它可以是一个对象, 一个组件, 一个服务, 或者一台机器, 在处理不同的问题以及使用不同的分布式编程模式时, Node有着不同的意义.
+ Link: Node之间能过Link进行连接, 一般而言, 这些Link是基于TCP/IP的. 当然, 在一些特殊的系统中, 为了实现更高性能的传输, 可能会使用其他的传输方式.

### 两个特性
相对于单机系统, 分布式系统天生具有高可伸缩性的特点, 这使得它理论上可以应对无限的负载, 只要提供足够的资源. 然而, 这并不是免费的. 无限的可伸缩性带来的一个负面影响就是整个系统的故障率增高. 相对于单机系统, 我们无法在忽略系统出错的可能, 相反, 我们需要详尽考虑如何处理系统中可能出现的错误. 因此, 我们将 **伸缩** 与 **容错** 定义为分布式系统的两个本质特征. 为了了解一个分布式系统, 我们将对这两个特征分别进行研究. 考虑到复杂性问题, 我们只考虑Node的伸缩以及容错问题.

### 两个原语
在了解了分布式系统的两个特征后, 我们再来考虑, 这两个特征对于我们的系统意味着什么, 我们如何实现及处理这两个特征. 在此, 我们先定义分布式系统中的两个原语: **Partition**, **Replication**. 在之后的说明中, 我们将详细阐释两个原语是如何解决分布式系统中的种种问题的.

## Scalability
典型的, 处理伸缩问题有两种方案: 水平伸缩以及垂直伸缩. 在一个复杂的系统中, 这两种方案会被同时使用. 其中, 垂直伸缩的方案较为直观, 主流的云平台均已提供. 在此, 我们将着重阐释水平伸缩的实现. 下文所说的伸缩, 一律指水平伸缩.

在分布式系统中, 假设我们有若干任务需要处理, 其数量为M. 开始时, 一台机器就已经能够处理它. 如果任务的规模提高了1k倍, 变成1000M, 如何保证我们的系统能够处理它? 显然, 我们需要更多的机器. 同时, 我们需要对任务进行Partition, 将其分成N份. 并将这N份任务分配(Placement)到已有的机器上. 于是, 每个机器只需要处理相对较小的一部分任务, 而这显然在其处理范围之内. 当其无法处理时, 我们可以重复使用以上的策略.

考虑分布式系统中两种典型的负载: Batch以及Stream.

+ 在一个Batch任务中, 所有需要进行的计算已知. 我们只需要将整个任务进行Partition, 然后Place到对应的机器上. 这是MapReduce的作法.
+ 在一个Stream任务中, 请求一条一条地进入我们的系统, 我们需要保证无论请求数量有多大(比如一秒来100M个请求), 系统都需要能够处理. 对于每一个请求, 我们需要将其 **Route** 到对应的机器上.

Batch任务的处理比较直观. 接下来我们主要考虑Stream任务.

首先, 我们先给出几个定义:

+ Partition: f: taskID -> PartitionID
+ Placement: f: PartitionID -> nodeID
+ Route: f: taskID -> nodeID

一个显然的问题是, 为什么要区分Partition与Placement? 我们直接使用Partition函数, 将task映射到node上不行吗? 区分Partition与Placement, 我们实际上增加了一层抽象, 那么, 这层抽象到底解决了什么问题? 考虑一个MySQL Sharding方案. 最初系统有三台机器, 我们对record的ID模三, 将其映射到三台机器上. 这工作的很好. 但是考虑, 如果此时我们增加了一台MySQL, 会发生什么? 一个直观的想法是将recordID模四, 然后映射到四台机器上. 此时, 所有已有的数据, 全部需要迁移! 相反, 我们加一层抽象, 将数据映射到逻辑分区上, 再将逻辑分区映射到物理机器上. 此时, 当机器增加时, 我们只需要将部分逻辑分区迁移到新的机器上, 其他逻辑分区完成不需要迁移! 在第一种方案中, task是一个固定大小的集合, node集合是一个变量, node的变更会导致整个方案的变更, 无数的task都需要调整. 而在使用第二种方案时, node的变更只会导致partition位置的变更, 而partition集合的大小远远小于task集合. 因此也就意味着处理变更的操作会简单许多. 再比如, Bigtable是以Tablet为单位进行调度, 而不是以row为单位进行调度.

考虑到整个流程的工作是将请求映射到对应的Node上, 我们将其过程称之为Routing.

### 两类系统的Partition
那么, 我们如何处理Partition问题呢? **解决一个问题最好的方法就是: 不解决它**. 对于任意一个task, 我们将任意它放置到一个Node上. 此时, 一个简单的LB就可以解决我们的工作. 不解决这个问题, 那么我们的系统正确性怎么办? 需要注意的是, 当且仅当task的执行是node相关时, 我们需要路由. 如果task在任意一台机器上执行是等价的, 则直接LB就已经足够.  显然, 当系统无状态时, task在任意node上时执行效果等价. 而当系统有状态时, 如DataStore, 问题才会复杂起来.

那么, 对于有状态的系统, 问题应该如何处理呢? 为了让问题更加直观一些, 假设我们需要对一个DataStore进行Partition. 每一个Request访问特定item, 每个key由一个唯一ID标识. 由于此类问题比较复杂, 且在不同场景下解决方式也有很大不同, 我们不妨看一些例子:

+ BitTorrent: 由一个TrackServer作为路由表, 当需要访问一个对象时, 直接向TrackServer查找所有对应的Node.
+ GnuTella:
    + 版本1: 无路由表, 所有对象的路由使用Flooding方式完成
    + 版本2: 局部路由表, 若干主干节点维护局部路由表, 主干节点之前通过Gossip进行路由解析
+ AFS: 所以节点都维护整个路由表, 虽然每个节点只存储部分数据
+ GFS: Master维护路由表, 创建/删除操作都会更新路由表, 读取操作会访问路由表
+ Bigtable: 同样使用中心路由表
+ Mongodb: 由config server维护路由表
+ Pastry: 分布式O(N)路由协议, 无中心路由表
+ Dynamo: 分布式O(1)路由协议, 无中心路由表

通过上面这些例子, 我们可以看到, Partition方案可以有以下的权衡方案:

+ 有无路由表
+ 有无Master

对上面的例子做一个小结. 可以看到, Partition方案的分类大致如下.

+ 中心路由表
    + 特点: 有一张中心路由表, 一般为主从式. 客户端进行query时通过路由表来找到提供服务的node
    + 例子:
        + GFS
        + Bigtable
        + BitTorrent: 对于单个种子, 它的Tracker可以看作是主, 虽然会有多个Tracker
        + Mongodb
+ 无中心路由表
    + 特点: 没有中心路由表. 各Node维护自己的路由表. 形式非常多
    + 分类: 按照结构化的程度, 此类系统还可以进一步分
        + 非结构化: Gnutella v1. 完全没有路由表或者任何结构, 路由需要通过Flooding来进行.
        + 半结构化: Gnutella v2/Skyper. 工作节点连入主干节点, 主干节点维护部分路由表. 主干节点之前使用Gossip路由.
        + 结构化:
            + Dynamo. 节点严格组织, 数据的分布严格组织. 按照此组织进行路由.
            + AFS/Coda. 节点松耦合. 每个节点维护一个完整的路由表. 这是上个世纪的系统, 比较粗糙.

不同的选择方案会带来不同的系统性质. 由于Partition方案的复杂性, 这里给出的分类并不是一个详尽的分类, 只是给出一个直观的分类, 让大家在遇到或者设计更加复杂方案时有一个基础出发点, 避免毫无头绪. 在之后的实例分析中, 我们将继续看一些具体的实现方法.

## Fault Tolerance
解决了可伸缩性问题后, 第二个问题出现了, 如何处理错误. 由于错误的范围很大, 在些, 我们只考虑一个小的问题: **如何保证我们的系统总是可用的**. 沿着上面的思路, 我们考虑无状态应用与有状态应用. 对于无状态应用, 问题很简单, 一个节点挂了, 我们访问另外一个有同样功能的节点. 这很直观. 于是, 分布式系统中的第二个原语出现了: Replication(或者更加通用的: Redundancy). 将这种思路应用到有状态系统中, 如果一个节点挂了, 我们访问另一个有相同状态的节点就是了. 相比较无状态的Replication, 有状态时问题更加复杂一些: 我们如何保证两个节点状态相同? 我们更新一个节点的数据时, 如何对其他具有相同状态的节点进行同步? 在此, 我们先不考虑节点恢复的问题.

在考虑状态同步时, 一个分布式系统中非常重要的问题出现了: 一致性问题.

Consistency这个词在计算机系统中已经用烂了, 然而麻烦的时, 在不同的场景下, 这个词的语义并不完全相同. 在此, 我们只考虑:

+ **分布式DataStore中**
+ **单个对象** 的 **多个副本** 中更新的可见性与顺序问题

上面的定义还是有些抽象, 我并不准备从学术性比较强的角度来讲这个问题, 相反, 我们从例子的角度出发. 首先, 给出一个学术味道不太浓的定义:

+ **它是Replication的直接产物**：一个Fact有N个View
+ **正常流**: 当对Fact进行Mutation后，N个View反映出这种Mutation的时间及顺序。
+ **异常流**: 如果Mutation后，N个View会有一个不一致性区间，此时，如果某个View失效了，则N个View
    + 能否达到相同的状态
    + 何时达到相同的状态

由于一致性问题是Replication的直接产物，我们要先来研究Replication问题，之后，研究在不同Replication场景下，一致性是如何达到的。

### Replication
Replication是分布式系统实现容错的最主要手段。首先，我们定义Replication的节点的类别：

+ 主备份：可以接受读写请求的备份。它是一个Fact的直接体现。它的Mutation就是Fact的Mutation。
+ 从备份：不接受读写，或者只能接受读请求。它是一个Fact的冗余View，它可以反应着当前Fact，也可以反应着一个较老的Fact。
    + 不接受读写：仅作 **冗余**，当主节点失效时，自动将此节点切换为主节点
    + 接受读请求：也用于冗余。但其有性能上的考虑，可以帮助主节点分担部分读请求。由于其View反应得可能不是最新的Fact，因此，可能读取到旧的值。

不同的备份策略，会对系统的性能/可用性/一致性都造成极大的影响。下面，定义Replication的分类及其优劣：

+ 主主备份
    + 性质：
        + 可用性: 这种情况下系统可用性最高，任何节点失效，都不影响整个系统的正常使用。
        + 性能: 由于多个节点都可以接受写请求，其并发程序相对最高
        + 冲突解析：由于有多个主，相当于一个Fact有多个版本，因此同步时要做冲突解析。
    + 分类：
        + N个主节点同时接受读写，并实时同步状态。适用于集群内同步。
        + N个主节点同时接受读写，并异步同步状态。适用于异地同步。
+ 主从备份
    + 性质：
        + 可用性：主节点失效后，需要切换从节点为主节点，可能会有状态损失。
        + 性能：仅仅是主节点承担读写压力。有时会用从节点分担读压力。
        + 冲突解析：由于只有一个主，因此同步时不需要做冲突解析。
    + 分类：
        + 主从实时同步：主节点写入自身后，还需要写入所有从节点，操作才算完成。
            + 可用性：可用性高，主节点失效后可立即切换到从节点
            + 容错性：不会有状态损失
            + 性能：性能会差一些。
        + 主从异步同步：主节点写入自身后，异步同步到从节点。
            + 可用性：可用性高，主节点失效后可立即切换到从节点，会有状态损失
            + 容错性：会有状态损失
            + 性能：相对较好

定义了四种Replication的方法，下面，考虑它的适用场景，以及不同Replication方式下，一致性问题的解决。

### 一致性协议
一致性协议，定义了Fact在经历Mutation后，如何达到一致的状态（**正常流**），以及，在有错误出现的情况下，如何达到一致的状态（**异常流**）。

#### 最简单的一致性协议
在考虑四种Replication场景下的一致性协议前，先考虑最简单的一致性协议是什么？**解决一个问题最好的方法就是，不解决它**。如果系统中不存在Replication，则我们根本不需要考虑一致性问题。一个典型的分布式系统的例子是，BigTable。每个Tablet只有一个副本，根据不需要考虑它的一致性问题。显然，这种场景非常好。那么，为什么还需要其他的方案呢？显然，这种方案有它的不足。BigTable中，Tablet是可以容错的，是持久化的，但是：

+ 性能：当并发量上去以后，单个Tablet副本是否可以支持？
+ 可用性：当负责此Tablet的Server失效以后，在此Tablet在其他Server上工作起来之前，有一段的不可用区间。

#### 主从实时同步场景下的一致性协议
那么，我们如何解决上面两个问题呢？对于第一个问题，显然，我们只需要增加副本就可以了，为了简单，我们增加若干只读从副本（若增加可写副本，需要考虑冲突问题，太复杂，先不考虑），它们可以分担读的压力，从而实现读写分离。对于第二个问题，还是为了简单，我们对主副本及若干只读副本做实时同步。此时，考虑我们的一致性协议：

+ 在常规写操作下：写操作完成后，系统状态处于一致，不需要额外的工作
+ 有异常发生时：由于一个写操作前后，多个副本的状态都是处于一致的，因此不存在不一致区间，也就不需要考虑容错问题

这种场景的一个典型的例子就是GFS. 在GFS中，对于特定Chunk的N个副本，会有一个主Chunk，其他的都是从Chunk，所有Chunk都可以接受读请求，但进行写操作时，必须由主Chunk进行协调，将写操作同步到所有的从Chunk上。只有所有Chunk上的更新都成功了，整个写操作才算成功。

显然，这种方案也很好了。那么，为什么又需要其他的方案呢？显然，它也有它的问题。

+ 性能: 每次写操作必须要写入所有副本才能返回,这太慢了!我们需要更高的呑吐.
+ 可用性: 在一个写操作过程中, 当主副本当从副本X发送同步请求, 同步操作可能超时. 而此时, 主副本无法判断, 是因此X挂了, 还是由于网络太差. 于是, 它唯一能做的, 只有重试(如果它直接返回, 而X又还没有挂, 那么, 此时, 就会副本X就会和主副本不一致, 违反了系统的不变式). 这会造成写操作的延迟进一步上升.

第一个问题是此种Replication方案下无法避免的. 但是第二个问题, 我们可以通过改进一致性协议, 来解决它.

+ 如果对X进行N个重试操作后, 操作依然没有完成, 那么, 将其标识为Failed, 请求结束. 副本X将不允许再接受其他的读请求.
+ 修复方案: 分为两种情况
    + 完全丢弃其副本. 于是此Chunk的副本数-1. 之后, 将由Replication机制来增加其副本.
    + 修复此副本. 当此副本X再次可用时, 由其他副本对其进行同步.
        + 如果同步机制比较复杂, 比如, 我们支持随机写, 则Chunk中很多地方都会出现不同, 此时, 必须复制比较整个Chunk, 才能判断如果同步. 这种情况下, 完全丢弃此副本, 由Replication机制来解决是一个更好的方法
        + 如果Chunk是Append-only的, 则比较其长度, 复制额外的内容即可.

#### 主从异步同步场景下的一致性协议
观察上面两个问题, 显然, 都是因为要做实时同步而引起的, 因此, 我们去掉此条件, 使用异步一致性. 此种情况下, 一致性协议是什么样的呢?

+ 正常流: 操作在主副本完成后, 返回客户端, 并将操作异步同步到从副本. 此种场景下, 我们达到了 **最终一致性**.
+ 异常流: 假设由主副本A, 两个从副本X/Y. 由以下几种情况
    + X/Y都没有完成同步, A挂了. 此时, 从X/Y中选出新的主副本. 丢失了一部分状态.
    + 系统还没有完成同步, X或Y挂了, 系统正常工作
    + X完成同步, Y还没有完成同步, A挂了. 从X/Y中选出新的主副本, 并进行 **冲突修复**. 一般来说, 会选择X为新副本, 并由其将状态同步到Y.

显然, 这种场景下, 系统只有最终一致性. 并且, 在从副本读取数据时, 可能并不是实时的. 有时, 为了避免这种非实时, 我们不允许从副本接受读请求, 仅仅将其作为容错机制.

这种一致性协议广泛用于MySQL主从备份+读写分离. 然而, 需要意识到的是, 它本身是一种妥协, 是在性能与可用性之间的妥协.

于是, 同样的问题来了, 为什么我们还需要主主同步呢? 分析主从异步同步方案:

+ 性能: 写操作全部需要由单个主副本完成, 显然, 其并发量是有上限的. 如果任意副本都可以写, 那么系统的并发量又可以上一个台阶.
+ 可用性: 主节点挂掉后, 我们需要一定的时间, 来选出一个新的主节点, 这会损失一些可用性
+ 可伸缩性: 当系统的规模非常大时, 比如, 有多个数据中心, 美国一个, 中国一个, 此时, 所以写操作都流向一个主节点, 每个请求都要跨越一次太平洋, 速度太慢!

此时, 我们需要多主节点备份机制. 在这种场景下, 可用性空前提高, 但是一致性问题变得极其复杂.

#### 主主实时同步场景下的一致性协议
主主同步问题十分复杂, 为了简单, 先考虑主主实时同步. 不过需要注意的是, 由于主主备份通常都是为了高性能/高可用, 而使用实时同步又将性能降了下来, 大量的分布式锁的出现会极大地影响整个的性能. 所以这种系统的可伸缩性及性能都会是一个很大的问题. 目前我所知道的这样的系统, 应该就只有数据库热备方案. 但这种方案只能在很小的范围内使用, 提供了极高的容错性及一致性.

#### 主主异步同步场景下的一致性协议
当我们的系统非常大时, 需要跨数据中心达成一致性, 此时, 所有写操作都有一个副本来处理显然无法满足伸缩性要求. 我们需要多主可写. 同时, 为了性能, 我们只能放弃一定的一致性, 多主之间异步同步以及冲突解析.

主主异步同步有一种典型的方案: `NWR`

+ 对于某个对象, 我们有N个副本
+ 写操作时, 需要至少写入W个对象, 操作才算成功
+ 读操作时, 需要至少读取R个对象, 并选出最新的对象, 操作才算成功

很显然, 我们必须有`W+R>N`, 否则, 无法保证可以读取到最新的写入. 于是, 对于某个N, W/R的选择, 与系统的读写性能以及可用性直接相关. 当我们需要高速写操作时, 可以令`W=1`, 此时, 只要有一个副本中写操作成功, 用户请求即可以返回. 相当的, `R=N`, 否则无法判断读到的值是否是最新的. 同时, 这种场景对于系统的容错是一个风险, 因此, 写入操作仅有一个成功, 其他无法保证, 如果其他的失败了, 同时, 已经写入的那么副本也失效了, 则这个更新就丢失了. 作为权衡, 我们可以令`W=2`或者`W=3`.

考虑其一致性协议:

+ 正常流:
    + 用户发来请求, 由一个协调者发送N条写请求, 当收到W条(包括协调者)写入成功的反馈时, 请求成功. 其他的副本会达到最终一致性.
+ 异常流: 理论上正常流会达到最终一致性. 但如果一个操作中网络发生故障, 或者节点发生故障, 则会出现不一致的情况
    + 读请求时, 由协调者发送N个读请求, 当收到R条时, 进行冲突解析, 确定一个值, 返回给用户, 并启动修复程序(仅存R条值不同的情况下), 通知相关副本此最新值.

此场景中, 出现了一个之前没有出现过的问题, 在读操作时, 一个副本可能有多个值, 如何确定哪一个值是最终结果, 显然, 我们要选择出`最新`的版本. 而在分布式场景中, 没有全局时间, 所以我们使用向量时间来标记每个副本的写入时间. 然而, 向量时间也不是万能的, 由于并发写操作的存在使得向量时钟也无法解决问题. 此时, 需要客户端人工干涉, 选择出一个值, 同时, 将各副本的值更新为此值.

# 分布式数据库分析
在了解了分布式系统的基础知识之后, 我们来看一些实际的例子. 考虑到分布式系统中状态的处理是最复杂的部分, 我们来看一看分布式数据库的一些问题. 说到分布式数据库, 有几个名词不得不提: NoSQL, CAP, BASE/ACID.

## 相关名词
### NoSQL
嗯, 先来讲个笑话:

+ A: NoSQL是个啥?
+ B:
    + Before 1970s: we have NO SQL.
    + In 2000s: we need NO SQL.
    + In 2010s: we need Not Only SQL.
    + Nowadays: NO, we need SQL.

NoSQL是一个很取巧的东西, 为了适应分布式环境, 它将数据库领域几十年来积累的东西全部丢掉, 让数据库回退到了只有KV的原始时代. 嗯, MySQL的最低下一层就是一个Primary Key到Record的B+ Tree(如果我没有记错的话), 实际上也就是一个KV. 这带来的好处是显著的, 但问题同样是显著的. 尽管后来NoSQL又引入了更加复杂的数据模型/单行事务/二级索引/查询语言. 但是, 人们还是在怀念使用SQL的美好时光.

当然, 这些并不是重点.

### CAP
这个词我并不想多说, 因为它的定义最模糊/争议最广泛/理解最多样. 但是由于它的核心概念非常直观, 许多没有看到原始论文的人, 在人云亦云之后, 也开始觉得自己理解了这个名词, 然后继续继续将这种并不算是错误的概念传播给其他的人. 很不幸, 我也是这些人中的一员, 所以我并不打算讲述这个概念. 唯一需要PS一下的就是, 这里的C与ACID里面的C并不是一个C. 它俩有关系吗? 谁知道呢.

### BASE
同样, 延续这一系列名词的光荣传统, 这个词的意思也没有那么准确, 更多的像是生凑出来的, 没错, 它就是凑出来的!

那么, BASE是定义是啥呢? 从化学上来讲:

+ PH < 7: ACID
+ PH > 7: BASE

从计算机上来讲:

+ BA: Basically availability, 指系统的首先目标是可用性, 这就从设计目标上说的
+ S: Soft state, 指系统实现时使用Soft state, 这是从实现方法上说的
+ E: Eventual consistency, 指系统放弃强一致性, 使用最终一致, 这是从实现策略上说的

基本上BA/E就是CAP中, CA的取舍. 而S就是来打酱油的...然而实际上, 干货味道最浓的就只有S了.

总之, 这个名词实际上没有什么用, 它出现要早于CAP, 可以理解为人们在处理分布式状态问题时早期实践. 大家忽略它就是了.

## 系统分析
接下来, 我们实际分析一些系统, 看看它们是如何使用Partition与Replication这两个分布式系统的原语的.

### 只使用了Partition的系统: Bigtable
在我所知道的分布式DataStore系统中, 我觉得设计上第二好的是Bigtable. 它架构于GFS之上, 使用了GFS这个高可用/高可伸缩的存储引擎, 使得其整个实现以及异常的处理变得非常简洁. 简单的系统必然易于理解, 易于使用, 易于扩展, 易于Debug.

下面, 我将从两个维度, 四个层次来说明Bigtable的实现.

+ 分布系统维度: 提供可伸缩性支持
    + 成员协议层
        + 成员协议负责维护当前系统的所有的成员
        + Bigtable的成员协议由Chubby这个分布式基础服务来维护, 这使得系统的功能组件可以不重复成员管理以及失效检测的逻辑.
    + 路由协议层
        + 负责进行Partition/Placement/Routing工作
        + 基本调度单位是Tablet(也就是一个虚拟的Partition)
        + Master负责Partition及Placement, 同时更新路由表.
        + 路由表根指针由Chubby维护, 路由表本身存储于存储节点中
        + Client会使用Chubby中的路由表来进行路由, 且大部分时间, 路由表会被缓存在Client中
+ 存储系统维度: 提供存储能力支持
    + 存储引擎层: 以GFS为存储介质, 使用LSM进行存储
    + 存储模型层: 提供Table模型

### 只使用了Replication的系统: Redis
Redis是一个使用非常广泛的缓存数据库. 然而在其集群版本出来之前, 它主要是单机版本的. 同时, 为了支持高可用, Redis提供了主从备份的机制. 由于Redis优秀的建模能力以及性能, 它已经广泛用于于分布式缓存领域. 缓存系统的soft state性质可以使得它们很容易伸缩.

Redis优秀的地方在于其异步IO机制, 以及其提供的丰富的数据结构. 当然, 这是存储系统维度的事情, 在这里, 我们更关心分布式系统维度的性质. 作为一个死硬KISS分子, Redis的主从机制也很简洁, 主节点可读写, 并将更新通过log的形式发送到所有只读从节点. 从节点会定期向主节点发送心跳. 没错, 这里, Redis只使用了Replication, 通过主从异步备份的方式来提供了高可用, 同时又不会影响整体上的性质.

### 同时使用了两个原语的系统: Mongodb
Mongodb是一个很在趣的系统, 它在系统中提供了两个独立的机制Sharding与Replication, 分别用于实现可伸缩与容错性.

+ Replication: 假设你现在只有一个Mongo实例, 并且它已经能够满足你的需求了. 显然, 我们需要高可用. 于是Mongodb提供了第一个机制, Replica set. 一个Replica set包含多个存储相同数据的节点, 其中一个为主, 接受读写请求；其他为从, 接受写请求. 主节点会在接受更新请求时, 将更新同步到从节点. 其中, 同步更新的方式是可以配置的, 同步/异步/完成N个写等等. 不同的Replication策略会给出不同的更新性能, 以及相对应的一致性/持久性性质. 越激进的策略性能会越好. 但是当错误发生时, 系统会陷入不一致的状态, 甚至丢失数据.
    + 写到Buffer即返回
    + 写到Log即返回
    + 写到磁盘即返回
    + 写一个节点即返回
    + 写二个节点即返回
    + 写N个节点即返回
+ Sharding: Mongodb的Sharding机制是一个很典型的Partition原语的实现. 以下, 所以节点都不考虑失效问题.
    + 系统组件
        + Routers: 无状态节点, 用于路由用户的请求
        + ConfigServer: 存储节点, 负责存储路由表, 实际上就是一个Mongo replica set. 整个系统唯一.
        + ShardServers: 存储节点, 负责提供存储能力. 每个ShardServer是一个Mongo replica set. 系统中会有许多的ShardServer, 用于提供存储.
    + Partition模型
        + 每个Collection有一个唯一的Shard Key.
        + Partition: 将Shard Key映射为一个ChunkID
        + Placement: 每个ShardServer会负责存储若干Chunk
        + Routing: Router会使用ConfigServer中的路由信息, 将ShardKey映射成ShardServer, 并将请求转发给它
    + 注意: 路由表大小理论上有限制, 但是可以通过层次结构(即一个Mongo cluster来做ConfigServer)来避免它

### 同时使用了两个原语的系统: Dynamo
Dynamo是一个很高大上的分布式存储系统. 然而, 作为一个Unix教条主义者, 我并不喜欢这样的系统. 它过于复杂, 整个系统的状态很难推导. 这意味着, 整个系统很不透明:

+ 如果系统出了问题, 我很难较快地定位问题出在哪里, 以及为什么会有这个问题.
+ 如果需要对这个系统进行二次开发, 我必须对这个系统非常了解, 才能够达到这一点. 然而, 这个系统的复杂性意味着我想要完全了解它需要很大的精力

Dynamo做了一些决策, 使得它与当前其他存储系统具有很大的不同:

+ 成员协议层面, 它使用了P2P协议, 通过Gossip + Failure Detector来维护成员列表. 这带来了以下的性质
    + 理论上可伸缩能力更强了
    + 成员列表这一Truth, 被许多节点维护, 每个节点看到的是自己的列表.
    + 在错误发生时, 这个列表会有一个不一致区间, 每个节点看到的内容都不一样, 这使得系统的状态难以推导
+ Replication方面, 它允许多点写入.
    + 可用性高, SLA得到保证
    + 版本冲突: 需要应用的介入来解决
+ 存储引擎方面, 需要应用来解决无法自动解决的冲突
    + 使得存储模型的接口变得复杂
    + 应用数据的状态难以推导

下面, 分布从两个维度, 四个层次来简要说明此系统

#### 存储系统维度
+ 存储引擎: 可以配置的. 只要能够提供KV接口就行了. 比如Berkeyly DB, MySQL, 等等
+ 存储模型: 带版本的KV
    + 读操作时: Get(key) -> List(value, version), 需要冲突解析
    + 写操作时: Put(key, value, version), 在特定版本的基础上写

小结来说, SQL提供了最好用的数据模型, 但性能与可伸缩性都是问题. 一般NoSQL提供了一个不太好用的数据模型, 解决了一些问题. Dynamo进一步复杂化了数据模型, 但是提供了更好优秀的性能与可伸缩性. 所以, 你要哪个?

#### 分布式维度
+ 成员协议: 使用Gossip + Failure Detector
    + 每个节点完全等价
    + 每个节点通过Gossip来同步自己的成员列表
    + 每个节点通过FD来检测其他节点的可用性
+ 路由协议: 常量时间寻址DHT
    + 路由方案事先定义好了: Hash(key) -> PartitionID -> NodeID
    + 请求可以到任意节点, 此节点负责路由

Dynamo的一个特点是, 乍一看比较简单, 但是深入系统时, 会遇到无数问题, 然后每个问题都会引入新的机制来解决它. 然而, 这些无数技术复合起来之后, 基本没人能了解系统当前状态是啥了. 比如,

+ 新节点加入时, 其他节点如何知道它?
+ 当系统是由多个集群组成, 忽然集群之间的网络断了, 这个系统会怎能样? 一小时后, 又恢复了, 系统又会怎么样?
+ 当两个客户端同时写一个对象x, 当客户端1的请求被路由到Node1上, 客户端2的请求被路由到Node2上, Node1开始写x, Node2也开始写x, 之后, Node1将发送同步请求到Node2, 同样, Node2也发送了同步请求到Node1. 这次写的结果会是什么?
